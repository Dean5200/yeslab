{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验：对决网络勇闯死亡回廊\n",
    "\n",
    "## 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from vizdoom import *\n",
    "from skimage import transform\n",
    "from collections import deque\n",
    "\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建游戏环境\n",
    "* 死亡回廊游戏环境包含 7 个动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"deadly_corridor.cfg\")\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    possible_actions = np.identity(7,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 图像处理过程\n",
    "* 图像处理包含两个过程：处理环境给予的多个图像帧与处理每一帧\n",
    "\n",
    "### 如何处理每一帧\n",
    "* TODO:完成下述函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 参数：每一帧\n",
    "## 过程1：去除无用图像部分\n",
    "## 过程2：画面大小缩放到[100,120]或适合的大小（请你完成）\n",
    "\"\"\"\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    \n",
    "\"\"\"\n",
    "## 过程3：归一化（请你完成）\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[15:-5,20:-20]\n",
    "    \n",
    "    # 画面大小重新剪裁\n",
    "    preprocessed_frame = \n",
    "    \n",
    "    # 归一化\n",
    "    normalized_frame = \n",
    "    \n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何处理环境给予的多个图像\n",
    "* 环境每次依然给与4帧的图像\n",
    "* 完成图像打包\n",
    "* TODO：尝试优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "\n",
    "# 初始化队列\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)   \n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 设定超参数\n",
    "* 设定强化学习超参数\n",
    "* 设定深度学习超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数\n",
    "state_size = [100,120,4] #注意这里要配合前面的图像处理大小\n",
    "action_size = game.get_available_buttons_size()\n",
    "learning_rate =  0.00025\n",
    "\n",
    "# 训练参数\n",
    "total_episodes = 5000\n",
    "max_steps = 5000\n",
    "batch_size = 64\n",
    "\n",
    "# 强化学习算法参数 \n",
    "max_tau = 10000\n",
    "gamma = 0.95\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.00005\n",
    "\n",
    "# 记忆力参数\n",
    "pretrain_length = 10000\n",
    "memory_size = 1000000\n",
    "\n",
    "# 环境参数\n",
    "training = False\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 创建对决网络模型\n",
    "* 首先模型接收 4 帧图像作为数据\n",
    "* 然后经过 3 层卷积\n",
    "* 铺平\n",
    "* 接下来会分离两条计算路径\n",
    "    - 一条路计算 V(s)\n",
    "    - 另一条路计算 A(s,a)\n",
    "* 最后做汇聚\n",
    "* 模型返回 Q 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            # TODO: 建立输入、动作、target_Q这三个 placeholder\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            #\n",
    "            self.ISWeights_ = tf.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            \n",
    "            \n",
    "            # TODO：建立第一层卷积，COV+ELU，建议初始化参数使用 tf.contrib.layers.xavier_initializer_conv2d()\n",
    "            \n",
    "            \n",
    "            # TODO：建立第二层卷积，同上\n",
    "            \n",
    "            \n",
    "            # TODO：建立第三层卷积，同上\n",
    "            \n",
    "            \n",
    "            # TODO：建立铺平层\n",
    "            \n",
    "            \n",
    "            # TODO：使用建立两条全连接计算流\n",
    "            ## V(s)计算流第一个隐含层包含 512 个神经元，第二层是大小为 1 的输出\n",
    "            \n",
    "            \n",
    "            # A(s,a)计算流第一个隐含层包含 512 个神经元，第二层是大小为 动作范围 的输出\n",
    "            \n",
    "            \n",
    "            # TODO：按以下公式进行汇聚\n",
    "            ## Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = \n",
    "              \n",
    "            # 让上面输出与动作相乘，得到唯一的优选动作 Q\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # 以下内容请勿修改\n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# 更新求和树所需           \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q, self.Q))           \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上述模型完成后就可以进行实例化了\n",
    "tf.reset_default_graph()\n",
    "DQNetwork = DuelNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "TargetNetwork = DuelNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 记忆优化\n",
    "* 按照论文，我们使用了求和树作为存储记忆优先级的基本数据结构\n",
    "![sum_tree](assets\\SUM_TREE.png)\n",
    "\n",
    "* 求和树：\n",
    "    - **def __init__**: 初始化求和树，建立求和树结构与记忆结构\n",
    "    - **def add**: 把记忆与对应的优先级评分添加进来\n",
    "    - **def update**: 当记忆的优先级发生改变时可以进行对应的更新\n",
    "    - **def get_leaf**: 优先级查询\n",
    "    - **def total_priority**: 计算优先级总和\n",
    "    \n",
    "* 记忆过程\n",
    "    - **def __init__**: 实例化求和树\n",
    "    - **def store**: 存储经验\n",
    "    - **def sample**: 记忆采样\n",
    "    - **def update_batch**: 更新树的优先级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree():\n",
    "    data_pointer = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_index, priority)\n",
    "        self.data_pointer += 1       \n",
    "        if self.data_pointer >= self.capacity: \n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_index, priority):\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else:      \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index          \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index    \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "    \n",
    "\n",
    "class Memory():\n",
    "    PER_e = 0.01 # 防止 0 采样\n",
    "    PER_a = 0.6  # 高优先级采样概率\n",
    "    PER_b = 0.4  # 重要性采样\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, experience):\n",
    "        # 新的记忆拥有最高优先级\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_priority == 0:\n",
    "            max_priority = 1.\n",
    "        \n",
    "        self.tree.add(max_priority, experience)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        memory_b = []\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        # 将优先级进行分段\n",
    "        priority_segment = self.tree.total_priority / n\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])        \n",
    "        # 计算最大权重\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max_weight == (N*P(i))**-b  /max_weight\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight                                  \n",
    "            b_idx[i]= index           \n",
    "            experience = [data]          \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e\n",
    "        clipped_errors = np.minimum(abs_errors, 1.)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小插曲：初始记忆填充\n",
    "这里只做了随机动作填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game.init()\n",
    "\n",
    "memory = Memory(memory_size)\n",
    "game.new_episode()\n",
    "\n",
    "# pretrain_length 就是填充长度\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    action = random.choice(possible_actions)\n",
    "    reward = game.make_action(action)\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)       \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        state = next_state\n",
    "        \n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 建立日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"tensorboard/dddqn/1\")\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 准备训练\n",
    "- **predict_action**：贪心算法输出动作\n",
    "- **update_target_graph**：完成从行为网络到目标网络的参数拷贝工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph():\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "    op_holder = []\n",
    "\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -98.28929138183594 Training loss: 0.0000 Explore P: 0.9959\n",
      "Model Saved\n",
      "Episode: 1 Total reward: -111.12425231933594 Training loss: 0.0000 Explore P: 0.9918\n",
      "Episode: 2 Total reward: -77.59027099609375 Training loss: 0.0000 Explore P: 0.9878\n",
      "Episode: 3 Total reward: -108.08290100097656 Training loss: 0.0000 Explore P: 0.9854\n",
      "Episode: 4 Total reward: -113.75927734375 Training loss: 0.0000 Explore P: 0.9833\n",
      "Episode: 5 Total reward: -83.06974792480469 Training loss: 0.0000 Explore P: 0.9738\n",
      "Model Saved\n",
      "Episode: 6 Total reward: -81.75521850585938 Training loss: 0.0000 Explore P: 0.9699\n",
      "Episode: 7 Total reward: -81.36335754394531 Training loss: 0.0000 Explore P: 0.9661\n",
      "Episode: 8 Total reward: -109.69892883300781 Training loss: 0.0000 Explore P: 0.9622\n",
      "Episode: 9 Total reward: -112.72572326660156 Training loss: 0.0000 Explore P: 0.9583\n",
      "Episode: 10 Total reward: -108.84414672851562 Training loss: 0.0000 Explore P: 0.9545\n",
      "Model Saved\n",
      "Episode: 11 Total reward: -115.96955871582031 Training loss: 0.0000 Explore P: 0.9507\n",
      "Episode: 12 Total reward: -110.48837280273438 Training loss: 0.0000 Explore P: 0.9453\n",
      "Episode: 13 Total reward: -106.83285522460938 Training loss: 0.0000 Explore P: 0.9402\n",
      "Episode: 14 Total reward: -94.42558288574219 Training loss: 0.0000 Explore P: 0.9364\n",
      "Episode: 15 Total reward: -114.50950622558594 Training loss: 0.0000 Explore P: 0.9327\n",
      "Model Saved\n",
      "Episode: 16 Total reward: -114.094970703125 Training loss: 0.0000 Explore P: 0.9279\n",
      "Episode: 17 Total reward: -104.91630554199219 Training loss: 0.0000 Explore P: 0.9174\n",
      "Episode: 18 Total reward: -112.97419738769531 Training loss: 0.0000 Explore P: 0.9107\n",
      "Episode: 19 Total reward: -104.11630249023438 Training loss: 0.0000 Explore P: 0.9060\n",
      "Episode: 20 Total reward: -94.88064575195312 Training loss: 0.0000 Explore P: 0.9040\n",
      "Model Saved\n",
      "Episode: 21 Total reward: -111.072509765625 Training loss: 0.0000 Explore P: 0.8966\n",
      "Episode: 22 Total reward: -96.24674987792969 Training loss: 0.0000 Explore P: 0.8944\n",
      "Episode: 23 Total reward: -98.18998718261719 Training loss: 0.0000 Explore P: 0.8843\n",
      "Episode: 24 Total reward: -86.96055603027344 Training loss: 0.0000 Explore P: 0.8808\n",
      "Episode: 25 Total reward: -71.59123229980469 Training loss: 0.0000 Explore P: 0.8723\n",
      "Model Saved\n",
      "Episode: 26 Total reward: -79.3377685546875 Training loss: 0.0000 Explore P: 0.8625\n",
      "Episode: 27 Total reward: -77.03132629394531 Training loss: 0.0000 Explore P: 0.8518\n",
      "Episode: 28 Total reward: -114.91848754882812 Training loss: 0.0000 Explore P: 0.8485\n",
      "Episode: 29 Total reward: -73.62553405761719 Training loss: 0.0000 Explore P: 0.8392\n",
      "Episode: 30 Total reward: -105.08729553222656 Training loss: 0.0000 Explore P: 0.8245\n",
      "Model Saved\n",
      "Episode: 31 Total reward: -88.06298828125 Training loss: 0.0000 Explore P: 0.8056\n",
      "Episode: 32 Total reward: -93.79194641113281 Training loss: 0.0000 Explore P: 0.8015\n",
      "Episode: 33 Total reward: -104.31182861328125 Training loss: 0.0000 Explore P: 0.7926\n",
      "Episode: 34 Total reward: -99.83024597167969 Training loss: 0.0000 Explore P: 0.7909\n",
      "Episode: 35 Total reward: -67.93649291992188 Training loss: 0.0000 Explore P: 0.7877\n",
      "Model Saved\n",
      "Episode: 36 Total reward: -94.41365051269531 Training loss: 0.0000 Explore P: 0.7821\n",
      "Episode: 37 Total reward: -115.99278259277344 Training loss: 0.0000 Explore P: 0.7667\n",
      "Episode: 38 Total reward: -115.96205139160156 Training loss: 0.0000 Explore P: 0.7628\n",
      "Episode: 39 Total reward: -115.06640625 Training loss: 0.0000 Explore P: 0.7590\n",
      "Episode: 40 Total reward: -110.80363464355469 Training loss: 0.0000 Explore P: 0.7571\n",
      "Model Saved\n",
      "Episode: 41 Total reward: -94.91366577148438 Training loss: 0.0000 Explore P: 0.7541\n",
      "Episode: 42 Total reward: -109.75355529785156 Training loss: 0.0000 Explore P: 0.7510\n",
      "Episode: 43 Total reward: -90.41732788085938 Training loss: 0.0000 Explore P: 0.7426\n",
      "Episode: 44 Total reward: -115.9951171875 Training loss: 0.0000 Explore P: 0.7343\n",
      "Episode: 45 Total reward: -100.52789306640625 Training loss: 0.0000 Explore P: 0.7305\n",
      "Model Saved\n",
      "Episode: 46 Total reward: -89.87040710449219 Training loss: 0.0000 Explore P: 0.7287\n",
      "Episode: 47 Total reward: -113.04751586914062 Training loss: 0.0000 Explore P: 0.7239\n",
      "Episode: 48 Total reward: -115.9915771484375 Training loss: 0.0000 Explore P: 0.7179\n",
      "Episode: 49 Total reward: -93.4119873046875 Training loss: 0.0000 Explore P: 0.7150\n",
      "Episode: 50 Total reward: -115.16059875488281 Training loss: 0.0000 Explore P: 0.7111\n",
      "Model Saved\n",
      "Episode: 51 Total reward: -114.65879821777344 Training loss: 0.0000 Explore P: 0.7083\n",
      "Episode: 52 Total reward: -103.98194885253906 Training loss: 0.0000 Explore P: 0.7046\n",
      "Episode: 53 Total reward: -98.42710876464844 Training loss: 0.0000 Explore P: 0.6979\n",
      "Episode: 54 Total reward: -115.95487976074219 Training loss: 0.0000 Explore P: 0.6901\n",
      "Episode: 55 Total reward: -115.8155517578125 Training loss: 0.0000 Explore P: 0.6834\n",
      "Model Saved\n",
      "Episode: 56 Total reward: -113.2469482421875 Training loss: 0.0000 Explore P: 0.6757\n",
      "Episode: 57 Total reward: -110.76824951171875 Training loss: 0.0000 Explore P: 0.6683\n",
      "Episode: 58 Total reward: -115.72021484375 Training loss: 0.0000 Explore P: 0.6655\n",
      "Episode: 59 Total reward: -97.33406066894531 Training loss: 0.0000 Explore P: 0.6641\n",
      "Episode: 60 Total reward: -115.99090576171875 Training loss: 0.0000 Explore P: 0.6566\n",
      "Model Saved\n",
      "Episode: 61 Total reward: -72.19488525390625 Training loss: 0.0000 Explore P: 0.6542\n",
      "Episode: 62 Total reward: -115.97671508789062 Training loss: 0.0000 Explore P: 0.6527\n",
      "Episode: 63 Total reward: -101.16049194335938 Training loss: 0.0000 Explore P: 0.6379\n",
      "Episode: 64 Total reward: -113.57975769042969 Training loss: 0.0000 Explore P: 0.6319\n",
      "Episode: 65 Total reward: -105.39225769042969 Training loss: 0.0000 Explore P: 0.6293\n",
      "Model Saved\n",
      "Episode: 66 Total reward: -114.95327758789062 Training loss: 0.0000 Explore P: 0.6150\n",
      "Model updated\n",
      "Episode: 67 Total reward: -115.26472473144531 Training loss: 0.0000 Explore P: 0.6091\n",
      "Episode: 68 Total reward: -113.58807373046875 Training loss: 0.0000 Explore P: 0.6066\n",
      "Episode: 69 Total reward: -103.91407775878906 Training loss: 0.0000 Explore P: 0.6042\n",
      "Episode: 70 Total reward: -110.47088623046875 Training loss: 0.0000 Explore P: 0.5904\n",
      "Model Saved\n",
      "Episode: 71 Total reward: -115.98492431640625 Training loss: 0.0000 Explore P: 0.5846\n",
      "Episode: 72 Total reward: -104.58233642578125 Training loss: 0.0000 Explore P: 0.5782\n",
      "Episode: 73 Total reward: -102.05274963378906 Training loss: 0.0000 Explore P: 0.5717\n",
      "Episode: 74 Total reward: -115.92930603027344 Training loss: 0.0000 Explore P: 0.5697\n",
      "Episode: 75 Total reward: -114.81634521484375 Training loss: 0.0000 Explore P: 0.5634\n",
      "Model Saved\n",
      "Episode: 76 Total reward: -109.25558471679688 Training loss: 0.0000 Explore P: 0.5505\n",
      "Episode: 77 Total reward: -115.98959350585938 Training loss: 0.0000 Explore P: 0.5475\n",
      "Episode: 78 Total reward: -106.71540832519531 Training loss: 0.0000 Explore P: 0.5461\n",
      "Episode: 79 Total reward: -98.73764038085938 Training loss: 0.0000 Explore P: 0.5398\n",
      "Episode: 80 Total reward: -109.46267700195312 Training loss: 0.0000 Explore P: 0.5378\n",
      "Model Saved\n",
      "Episode: 81 Total reward: -110.55723571777344 Training loss: 0.0000 Explore P: 0.5357\n",
      "Episode: 82 Total reward: -110.3280029296875 Training loss: 0.0000 Explore P: 0.5335\n",
      "Episode: 83 Total reward: -115.41119384765625 Training loss: 0.0000 Explore P: 0.5285\n",
      "Episode: 84 Total reward: -61.35163879394531 Training loss: 0.0000 Explore P: 0.5264\n",
      "Episode: 85 Total reward: -115.342529296875 Training loss: 0.0000 Explore P: 0.5244\n",
      "Model Saved\n",
      "Episode: 86 Total reward: -112.771240234375 Training loss: 0.0000 Explore P: 0.5222\n",
      "Episode: 87 Total reward: -111.97962951660156 Training loss: 0.0000 Explore P: 0.5171\n",
      "Episode: 88 Total reward: -107.56330871582031 Training loss: 0.0000 Explore P: 0.5114\n",
      "Episode: 89 Total reward: -115.99092102050781 Training loss: 0.0000 Explore P: 0.5093\n",
      "Episode: 90 Total reward: -80.48570251464844 Training loss: 0.0000 Explore P: 0.5073\n",
      "Model Saved\n",
      "Episode: 91 Total reward: -104.94961547851562 Training loss: 0.0000 Explore P: 0.5033\n",
      "Episode: 92 Total reward: -105.36398315429688 Training loss: 0.0000 Explore P: 0.4985\n",
      "Episode: 93 Total reward: -114.87901306152344 Training loss: 0.0000 Explore P: 0.4929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 94 Total reward: -105.9405517578125 Training loss: 0.0000 Explore P: 0.4909\n",
      "Episode: 95 Total reward: -115.69053649902344 Training loss: 0.0000 Explore P: 0.4854\n",
      "Model Saved\n",
      "Episode: 96 Total reward: -110.97093200683594 Training loss: 0.0000 Explore P: 0.4834\n",
      "Episode: 97 Total reward: -102.33390808105469 Training loss: 0.0000 Explore P: 0.4815\n",
      "Episode: 98 Total reward: -97.72427368164062 Training loss: 0.0000 Explore P: 0.4756\n",
      "Episode: 99 Total reward: -107.85105895996094 Training loss: 0.0000 Explore P: 0.4661\n",
      "Episode: 100 Total reward: -115.99830627441406 Training loss: 0.0000 Explore P: 0.4555\n",
      "Model Saved\n",
      "Episode: 101 Total reward: -115.97952270507812 Training loss: 0.0000 Explore P: 0.4477\n",
      "Episode: 102 Total reward: -111.42926025390625 Training loss: 0.0000 Explore P: 0.4459\n",
      "Episode: 103 Total reward: -109.58932495117188 Training loss: 0.0000 Explore P: 0.4411\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e157d399390c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mtree_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mISWeights_mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mstates_mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[0mactions_mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0mrewards_mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        tau = 0\n",
    "        game.init()\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                tau += 1\n",
    "                decay_step +=1\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    next_state = np.zeros((120,140), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    step = max_steps\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    # 记忆存储\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    # 记忆存储\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                # 训练部分     \n",
    "                ## 获取记忆的mini_batch\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                \n",
    "                ## 从行为网络获得 Q(s',a') 值\n",
    "                q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                ## 从目标网络获得 Q(s',a') 值\n",
    "                q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    ## 获取a'\n",
    "                    action = np.argmax(q_next_state[i])\n",
    "\n",
    "                    ## 如果是最终状态那么 Q_target = r\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    \n",
    "                    else:\n",
    "                        ## TODO: 实现 Double DQN 的 target 算法\n",
    "                        target = \n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                               DQNetwork.target_Q: targets_mb,\n",
    "                                                               DQNetwork.actions_: actions_mb,\n",
    "                                                               DQNetwork.ISWeights_: ISWeights_mb})\n",
    "              \n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "                \n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                        DQNetwork.target_Q: targets_mb,\n",
    "                                                        DQNetwork.actions_: actions_mb,\n",
    "                                                        DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                if tau > max_tau:\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Model updated\")\n",
    "\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")\n",
    "                \n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: 观看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from g_model/model.ckpt\n",
      "Score:  -79.04835510253906\n",
      "Score:  -99.02684020996094\n",
      "Score:  -77.61373901367188\n",
      "Score:  -115.29901123046875\n",
      "Score:  -54.537841796875\n",
      "Score:  -103.49974060058594\n",
      "Score:  -111.79505920410156\n",
      "Score:  -87.97618103027344\n",
      "Score:  -115.89765930175781\n",
      "Score:  -64.21710205078125\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    game.load_config(\"deadly_corridor_testing.cfg\")\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    saver.restore(sess, \"g_model/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            explore_probability = 0.01  \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                action = random.choice(possible_actions)\n",
    "        \n",
    "            else:\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
