{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验：对决网络勇闯死亡回廊\n",
    "\n",
    "## 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from vizdoom import *\n",
    "from skimage import transform\n",
    "from collections import deque\n",
    "\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建游戏环境\n",
    "* 死亡回廊游戏环境包含 7 个动作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"deadly_corridor.cfg\")\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    possible_actions = np.identity(7,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 图像处理过程\n",
    "* 图像处理包含两个过程：处理环境给予的多个图像帧与处理每一帧\n",
    "\n",
    "### 如何处理每一帧\n",
    "* TODO:完成下述函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 参数：每一帧\n",
    "## 过程1：去除无用图像部分\n",
    "## 过程2：画面大小缩放到[100,120]或适合的大小（请你完成）\n",
    "\"\"\"\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    \n",
    "\"\"\"\n",
    "## 过程3：归一化（请你完成）\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[15:-5,20:-20]\n",
    "    \n",
    "    # 画面大小重新剪裁\n",
    "    preprocessed_frame = transform.resize(cropped_frame, [100,120])\n",
    "    \n",
    "    # 归一化\n",
    "    normalized_frame = preprocessed_frame/255.0\n",
    "    \n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何处理环境给予的多个图像\n",
    "* 环境每次依然给与4帧的图像\n",
    "* 完成图像打包\n",
    "* TODO：尝试优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "\n",
    "# 初始化队列\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)   \n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 设定超参数\n",
    "* 设定强化学习超参数\n",
    "* 设定深度学习超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数\n",
    "state_size = [100,120,4] #注意这里要配合前面的图像处理大小\n",
    "action_size = game.get_available_buttons_size()\n",
    "learning_rate =  0.00025\n",
    "\n",
    "# 训练参数\n",
    "total_episodes = 5000\n",
    "max_steps = 5000\n",
    "batch_size = 32\n",
    "\n",
    "# 强化学习算法参数 \n",
    "max_tau = 10000\n",
    "gamma = 0.95\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.00005\n",
    "\n",
    "# 记忆力参数\n",
    "## 如果 pretrain_length 与 memory_size 相差太小一开始 loss 会变 0\n",
    "pretrain_length = 10000 #建议尝试100000，如果内存卡死就可以换10000-50000之间\n",
    "memory_size = 10000 #建议尝试1000000，如果内存卡死就可以换100000-500000之间\n",
    "\n",
    "# 环境参数\n",
    "training = True\n",
    "#episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 创建对决网络模型\n",
    "* 首先模型接收 4 帧图像作为数据\n",
    "* 然后经过 3 层卷积\n",
    "* 铺平\n",
    "* 接下来会分离两条计算路径\n",
    "    - 一条路计算 V(s)\n",
    "    - 另一条路计算 A(s,a)\n",
    "* 最后做汇聚\n",
    "* 模型返回 Q 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            # TODO: 建立输入、动作、target_Q这三个 placeholder\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            #\n",
    "            self.ISWeights_ = tf.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            \n",
    "            \n",
    "            # TODO：建立第一层卷积，COV+ELU，建议初始化参数使用 tf.contrib.layers.xavier_initializer_conv2d()\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                          filters = 32,\n",
    "                                          kernel_size = [8,8],\n",
    "                                          strides = [4,4],\n",
    "                                          padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv1\")\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            # TODO：建立第二层卷积，同上\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                          filters = 64,\n",
    "                                          kernel_size = [4,4],\n",
    "                                          strides = [2,2],\n",
    "                                          padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv2\")\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "            \n",
    "            \n",
    "            # TODO：建立第三层卷积，同上\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                          filters = 128,\n",
    "                                          kernel_size = [4,4],\n",
    "                                          strides = [2,2],\n",
    "                                          padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name = \"conv3\")\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            \n",
    "            # TODO：建立铺平层\n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            \n",
    "            # TODO：使用建立两条全连接计算流\n",
    "            ## V(s)计算流第一个隐含层包含 512 个神经元，第二层是大小为 1 的输出\n",
    "            self.value_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                            units = 512,\n",
    "                                            activation = tf.nn.elu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                            name=\"value_fc\")\n",
    "            \n",
    "            self.value = tf.layers.dense(inputs = self.value_fc,\n",
    "                                         units = 1,\n",
    "                                         activation = None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name=\"value\")\n",
    "            \n",
    "            # A(s,a)计算流第一个隐含层包含 512 个神经元，第二层是大小为 动作范围 的输出\n",
    "            self.advantage_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                                units = 512,\n",
    "                                                activation = tf.nn.elu,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"advantage_fc\")\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs = self.advantage_fc,\n",
    "                                             units = self.action_size,\n",
    "                                             activation = None,\n",
    "                                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                             name=\"advantages\")\n",
    "            \n",
    "            # TODO：按以下公式进行汇聚\n",
    "            ## Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "              \n",
    "            # 让上面输出与动作相乘，得到唯一的优选动作 Q\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # 以下内容请勿修改\n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# 更新求和树所需           \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q, self.Q))           \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上述模型完成后就可以进行实例化了\n",
    "tf.reset_default_graph()\n",
    "DQNetwork = DuelNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "TargetNetwork = DuelNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 记忆优化\n",
    "* 按照论文，我们使用了求和树作为存储记忆优先级的基本数据结构\n",
    "![sum_tree](assets\\SUM_TREE.png)\n",
    "\n",
    "* 求和树：\n",
    "    - **def __init__**: 初始化求和树，建立求和树结构与记忆结构\n",
    "    - **def add**: 把记忆与对应的优先级评分添加进来\n",
    "    - **def update**: 当记忆的优先级发生改变时可以进行对应的更新\n",
    "    - **def get_leaf**: 优先级查询\n",
    "    - **def total_priority**: 计算优先级总和\n",
    "    \n",
    "* 记忆过程\n",
    "    - **def __init__**: 实例化求和树\n",
    "    - **def store**: 存储经验\n",
    "    - **def sample**: 记忆采样\n",
    "    - **def update_batch**: 更新树的优先级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree():\n",
    "    data_pointer = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_index, priority)\n",
    "        self.data_pointer += 1       \n",
    "        if self.data_pointer >= self.capacity: \n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_index, priority):\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else:      \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index          \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index    \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "    \n",
    "\n",
    "class Memory():\n",
    "    PER_e = 0.01 # 防止 0 采样\n",
    "    PER_a = 0.6  # 高优先级采样概率\n",
    "    PER_b = 0.4  # 重要性采样\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, experience):\n",
    "        # 新的记忆拥有最高优先级\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_priority == 0:\n",
    "            max_priority = 1.\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "    def sample(self, n):\n",
    "        memory_b = []\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        # 将优先级进行分段\n",
    "        priority_segment = self.tree.total_priority / n\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])        \n",
    "        # 计算最大权重\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max_weight == (N*P(i))**-b  /max_weight\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight                                  \n",
    "            b_idx[i]= index           \n",
    "            experience = [data]          \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e\n",
    "        clipped_errors = np.minimum(abs_errors, 1.)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小插曲：初始记忆填充\n",
    "这里只做了随机动作填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game.init()\n",
    "\n",
    "memory = Memory(memory_size)\n",
    "game.new_episode()\n",
    "\n",
    "# pretrain_length 就是填充长度\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    action = random.choice(possible_actions)\n",
    "    reward = game.make_action(action)\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)       \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        state = next_state\n",
    "        \n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 建立日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"tensorboard/dddqn/1\")\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 准备训练\n",
    "- **predict_action**：贪心算法输出动作\n",
    "- **update_target_graph**：完成从行为网络到目标网络的参数拷贝工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph():\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "    op_holder = []\n",
    "\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -107.46109008789062 Training loss: 83.2708 Explore P: 0.9959\n",
      "Model Saved\n",
      "Episode: 1 Total reward: -99.21345520019531 Training loss: 0.5869 Explore P: 0.9904\n",
      "Episode: 2 Total reward: -92.57376098632812 Training loss: 0.5022 Explore P: 0.9864\n",
      "Episode: 3 Total reward: -106.30680847167969 Training loss: 0.2053 Explore P: 0.9829\n",
      "Episode: 4 Total reward: -107.10749816894531 Training loss: 38.8781 Explore P: 0.9778\n",
      "Episode: 5 Total reward: -93.06199645996094 Training loss: 0.1752 Explore P: 0.9739\n",
      "Model Saved\n",
      "Episode: 6 Total reward: -89.39212036132812 Training loss: 0.2813 Explore P: 0.9698\n",
      "Episode: 7 Total reward: -102.98558044433594 Training loss: 0.2687 Explore P: 0.9659\n",
      "Episode: 8 Total reward: -92.00413513183594 Training loss: 0.2565 Explore P: 0.9574\n",
      "Episode: 9 Total reward: -71.75691223144531 Training loss: 0.2212 Explore P: 0.9536\n",
      "Episode: 10 Total reward: -60.135284423828125 Training loss: 22.9234 Explore P: 0.9497\n",
      "Model Saved\n",
      "Episode: 11 Total reward: -52.80189514160156 Training loss: 21.1712 Explore P: 0.9474\n",
      "Episode: 12 Total reward: -115.22445678710938 Training loss: 0.0660 Explore P: 0.9451\n",
      "Episode: 13 Total reward: -96.23060607910156 Training loss: 0.1136 Explore P: 0.9426\n",
      "Episode: 14 Total reward: -85.35342407226562 Training loss: 0.1009 Explore P: 0.9388\n",
      "Episode: 15 Total reward: -89.85421752929688 Training loss: 0.2856 Explore P: 0.9350\n",
      "Model Saved\n",
      "Episode: 16 Total reward: -115.98068237304688 Training loss: 0.2526 Explore P: 0.9301\n",
      "Episode: 17 Total reward: -105.44020080566406 Training loss: 22.8526 Explore P: 0.9280\n",
      "Episode: 18 Total reward: -83.30644226074219 Training loss: 0.2581 Explore P: 0.9244\n",
      "Episode: 19 Total reward: -87.90965270996094 Training loss: 0.2294 Explore P: 0.9206\n",
      "Episode: 20 Total reward: -53.004669189453125 Training loss: 0.3328 Explore P: 0.9169\n",
      "Model Saved\n",
      "Episode: 21 Total reward: -90.72900390625 Training loss: 0.2413 Explore P: 0.9132\n",
      "Episode: 22 Total reward: -115.91511535644531 Training loss: 0.2436 Explore P: 0.9095\n",
      "Episode: 23 Total reward: -60.772430419921875 Training loss: 0.2999 Explore P: 0.9058\n",
      "Episode: 24 Total reward: -83.42227172851562 Training loss: 0.2501 Explore P: 0.9007\n",
      "Episode: 25 Total reward: -54.101806640625 Training loss: 0.1464 Explore P: 0.8971\n",
      "Model Saved\n",
      "Episode: 26 Total reward: -74.50379943847656 Training loss: 26.3432 Explore P: 0.8935\n",
      "Episode: 27 Total reward: -89.74946594238281 Training loss: 0.1728 Explore P: 0.8916\n",
      "Episode: 28 Total reward: -113.8499755859375 Training loss: 0.2677 Explore P: 0.8895\n",
      "Episode: 29 Total reward: -115.87472534179688 Training loss: 31.9558 Explore P: 0.8844\n",
      "Episode: 30 Total reward: -84.95146179199219 Training loss: 22.6842 Explore P: 0.8808\n",
      "Model Saved\n",
      "Episode: 31 Total reward: -38.42176818847656 Training loss: 21.6390 Explore P: 0.8772\n",
      "Episode: 32 Total reward: -76.33383178710938 Training loss: 20.9644 Explore P: 0.8737\n",
      "Episode: 33 Total reward: -51.36834716796875 Training loss: 20.8284 Explore P: 0.8701\n",
      "Episode: 34 Total reward: -85.72262573242188 Training loss: 25.5808 Explore P: 0.8644\n",
      "Episode: 35 Total reward: -115.33636474609375 Training loss: 20.4214 Explore P: 0.8599\n",
      "Model Saved\n",
      "Episode: 36 Total reward: -51.875640869140625 Training loss: 19.0450 Explore P: 0.8565\n",
      "Episode: 37 Total reward: -33.54522705078125 Training loss: 0.2556 Explore P: 0.8482\n",
      "Episode: 38 Total reward: -87.34605407714844 Training loss: 16.0891 Explore P: 0.8437\n",
      "Episode: 39 Total reward: -93.67727661132812 Training loss: 29.2367 Explore P: 0.8403\n",
      "Episode: 40 Total reward: -93.92291259765625 Training loss: 0.3794 Explore P: 0.8368\n",
      "Model Saved\n",
      "Episode: 41 Total reward: -79.85916137695312 Training loss: 0.2633 Explore P: 0.8310\n",
      "Episode: 42 Total reward: -0.259552001953125 Training loss: 28.9960 Explore P: 0.8250\n",
      "Episode: 43 Total reward: -61.035858154296875 Training loss: 0.6111 Explore P: 0.8216\n",
      "Episode: 44 Total reward: -84.30274963378906 Training loss: 0.4523 Explore P: 0.8185\n",
      "Episode: 45 Total reward: -106.34365844726562 Training loss: 0.6803 Explore P: 0.8167\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 47.750579833984375 Training loss: 0.4322 Explore P: 0.8042\n",
      "Episode: 47 Total reward: -72.65078735351562 Training loss: 0.4120 Explore P: 0.8022\n",
      "Episode: 48 Total reward: -49.418365478515625 Training loss: 18.9824 Explore P: 0.7990\n",
      "Episode: 49 Total reward: -75.39070129394531 Training loss: 0.7658 Explore P: 0.7962\n",
      "Episode: 50 Total reward: -49.67936706542969 Training loss: 33.4753 Explore P: 0.7929\n",
      "Model Saved\n",
      "Episode: 51 Total reward: -34.467529296875 Training loss: 0.8821 Explore P: 0.7896\n",
      "Episode: 52 Total reward: -98.50584411621094 Training loss: 0.3583 Explore P: 0.7850\n",
      "Episode: 53 Total reward: -43.32484436035156 Training loss: 0.8266 Explore P: 0.7806\n",
      "Episode: 54 Total reward: -57.31951904296875 Training loss: 26.0349 Explore P: 0.7775\n",
      "Episode: 55 Total reward: -34.268951416015625 Training loss: 0.7314 Explore P: 0.7743\n",
      "Model Saved\n",
      "Episode: 56 Total reward: -33.52732849121094 Training loss: 0.4252 Explore P: 0.7712\n",
      "Episode: 57 Total reward: -55.99931335449219 Training loss: 24.5562 Explore P: 0.7681\n",
      "Episode: 58 Total reward: -63.55867004394531 Training loss: 0.6620 Explore P: 0.7650\n",
      "Episode: 59 Total reward: -59.34614562988281 Training loss: 0.3411 Explore P: 0.7631\n",
      "Episode: 60 Total reward: -65.64356994628906 Training loss: 22.5321 Explore P: 0.7601\n",
      "Model Saved\n",
      "Episode: 61 Total reward: -82.75752258300781 Training loss: 0.7130 Explore P: 0.7583\n",
      "Episode: 62 Total reward: 17.89404296875 Training loss: 20.6723 Explore P: 0.7477\n",
      "Episode: 63 Total reward: -75.41128540039062 Training loss: 0.3696 Explore P: 0.7447\n",
      "Episode: 64 Total reward: -43.166412353515625 Training loss: 1.1167 Explore P: 0.7417\n",
      "Episode: 65 Total reward: -115.96136474609375 Training loss: 0.2755 Explore P: 0.7388\n",
      "Model Saved\n",
      "Episode: 66 Total reward: -69.42741394042969 Training loss: 0.3570 Explore P: 0.7323\n",
      "Episode: 67 Total reward: -62.500885009765625 Training loss: 0.4149 Explore P: 0.7282\n",
      "Episode: 68 Total reward: -40.16249084472656 Training loss: 0.4808 Explore P: 0.7242\n",
      "Episode: 69 Total reward: -48.78742980957031 Training loss: 23.0354 Explore P: 0.7207\n",
      "Episode: 70 Total reward: -14.5626220703125 Training loss: 0.3283 Explore P: 0.7170\n",
      "Model Saved\n",
      "Episode: 71 Total reward: -91.60714721679688 Training loss: 0.2017 Explore P: 0.7141\n",
      "Episode: 72 Total reward: -57.44615173339844 Training loss: 0.5223 Explore P: 0.7112\n",
      "Episode: 73 Total reward: -9.4493408203125 Training loss: 45.7578 Explore P: 0.7083\n",
      "Episode: 74 Total reward: -115.9757080078125 Training loss: 0.5154 Explore P: 0.7067\n",
      "Episode: 75 Total reward: -106.35348510742188 Training loss: 0.5898 Explore P: 0.7049\n",
      "Model Saved\n",
      "Episode: 76 Total reward: -27.190399169921875 Training loss: 49.8444 Explore P: 0.7010\n",
      "Episode: 77 Total reward: -57.40057373046875 Training loss: 0.4996 Explore P: 0.6982\n",
      "Episode: 78 Total reward: -94.43511962890625 Training loss: 0.9263 Explore P: 0.6954\n",
      "Episode: 79 Total reward: -27.537673950195312 Training loss: 0.4975 Explore P: 0.6926\n",
      "Episode: 80 Total reward: -89.00923156738281 Training loss: 0.6460 Explore P: 0.6898\n",
      "Model Saved\n",
      "Episode: 81 Total reward: -75.37124633789062 Training loss: 0.4849 Explore P: 0.6870\n",
      "Episode: 82 Total reward: -71.72749328613281 Training loss: 0.8222 Explore P: 0.6843\n",
      "Episode: 83 Total reward: -5.169921875 Training loss: 0.5714 Explore P: 0.6792\n",
      "Episode: 84 Total reward: -71.91529846191406 Training loss: 0.6906 Explore P: 0.6755\n",
      "Episode: 85 Total reward: -23.040771484375 Training loss: 22.0236 Explore P: 0.6727\n",
      "Model Saved\n",
      "Episode: 86 Total reward: -43.91310119628906 Training loss: 0.8519 Explore P: 0.6700\n",
      "Episode: 87 Total reward: -42.24272155761719 Training loss: 0.8525 Explore P: 0.6673\n",
      "Episode: 88 Total reward: -74.42349243164062 Training loss: 0.8302 Explore P: 0.6646\n",
      "Episode: 89 Total reward: -78.29058837890625 Training loss: 1.0158 Explore P: 0.6619\n",
      "Episode: 90 Total reward: -107.11471557617188 Training loss: 0.5276 Explore P: 0.6592\n",
      "Model Saved\n",
      "Episode: 91 Total reward: -105.0428466796875 Training loss: 38.9071 Explore P: 0.6566\n",
      "Episode: 92 Total reward: -59.45741271972656 Training loss: 1.0692 Explore P: 0.6530\n",
      "Episode: 93 Total reward: -23.143783569335938 Training loss: 0.3479 Explore P: 0.6503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 94 Total reward: -29.503646850585938 Training loss: 30.8521 Explore P: 0.6477\n",
      "Episode: 95 Total reward: -64.31242370605469 Training loss: 20.3328 Explore P: 0.6461\n",
      "Model Saved\n",
      "Episode: 96 Total reward: -115.9249267578125 Training loss: 0.6859 Explore P: 0.6435\n",
      "Episode: 97 Total reward: -79.70756530761719 Training loss: 0.8915 Explore P: 0.6408\n",
      "Episode: 98 Total reward: -55.674835205078125 Training loss: 0.4233 Explore P: 0.6382\n",
      "Episode: 99 Total reward: -18.534927368164062 Training loss: 21.2507 Explore P: 0.6358\n",
      "Episode: 100 Total reward: 10.925979614257812 Training loss: 0.6918 Explore P: 0.6333\n",
      "Model Saved\n",
      "Episode: 101 Total reward: -19.180633544921875 Training loss: 0.4938 Explore P: 0.6307\n",
      "Episode: 102 Total reward: -48.38787841796875 Training loss: 28.7360 Explore P: 0.6282\n",
      "Episode: 103 Total reward: -83.50558471679688 Training loss: 0.4542 Explore P: 0.6268\n",
      "Episode: 104 Total reward: -52.70941162109375 Training loss: 0.7229 Explore P: 0.6254\n",
      "Episode: 105 Total reward: -27.47796630859375 Training loss: 0.6688 Explore P: 0.6230\n",
      "Model Saved\n",
      "Episode: 106 Total reward: -70.625 Training loss: 0.7077 Explore P: 0.6196\n",
      "Episode: 107 Total reward: -32.50315856933594 Training loss: 0.9601 Explore P: 0.6171\n",
      "Episode: 108 Total reward: -44.62275695800781 Training loss: 0.5850 Explore P: 0.6150\n",
      "Episode: 109 Total reward: -33.88580322265625 Training loss: 24.0731 Explore P: 0.6125\n",
      "Model updated\n",
      "Episode: 110 Total reward: -63.74835205078125 Training loss: 0.7710 Explore P: 0.6101\n",
      "Model Saved\n",
      "Episode: 111 Total reward: -93.74490356445312 Training loss: 0.8421 Explore P: 0.6075\n",
      "Episode: 112 Total reward: -108.45553588867188 Training loss: 0.5790 Explore P: 0.6051\n",
      "Episode: 113 Total reward: -66.24111938476562 Training loss: 0.9150 Explore P: 0.5993\n",
      "Episode: 114 Total reward: -26.990234375 Training loss: 1.4111 Explore P: 0.5969\n",
      "Episode: 115 Total reward: -54.31755065917969 Training loss: 3.7782 Explore P: 0.5946\n",
      "Model Saved\n",
      "Episode: 116 Total reward: 22.807388305664062 Training loss: 22.2898 Explore P: 0.5923\n",
      "Episode: 117 Total reward: -16.878692626953125 Training loss: 1.1441 Explore P: 0.5891\n",
      "Episode: 118 Total reward: -5.001708984375 Training loss: 1.0371 Explore P: 0.5848\n",
      "Episode: 119 Total reward: -101.86312866210938 Training loss: 38.4965 Explore P: 0.5827\n",
      "Episode: 120 Total reward: -71.28697204589844 Training loss: 0.8749 Explore P: 0.5804\n",
      "Model Saved\n",
      "Episode: 121 Total reward: -58.75840759277344 Training loss: 0.4635 Explore P: 0.5780\n",
      "Episode: 122 Total reward: -47.81718444824219 Training loss: 0.6912 Explore P: 0.5757\n",
      "Episode: 123 Total reward: -7.16107177734375 Training loss: 0.5162 Explore P: 0.5734\n",
      "Episode: 124 Total reward: -4.8967132568359375 Training loss: 0.8346 Explore P: 0.5701\n",
      "Episode: 125 Total reward: 30.068099975585938 Training loss: 1.4668 Explore P: 0.5678\n",
      "Model Saved\n",
      "Episode: 126 Total reward: -55.43336486816406 Training loss: 0.5283 Explore P: 0.5655\n",
      "Episode: 127 Total reward: -47.66883850097656 Training loss: 0.9471 Explore P: 0.5632\n",
      "Episode: 128 Total reward: -53.145751953125 Training loss: 0.5178 Explore P: 0.5609\n",
      "Episode: 129 Total reward: -30.21844482421875 Training loss: 19.4883 Explore P: 0.5586\n",
      "Episode: 130 Total reward: -35.8907470703125 Training loss: 0.9536 Explore P: 0.5557\n",
      "Model Saved\n",
      "Episode: 131 Total reward: -11.67132568359375 Training loss: 24.2382 Explore P: 0.5534\n",
      "Episode: 132 Total reward: -63.85151672363281 Training loss: 0.9005 Explore P: 0.5512\n",
      "Episode: 133 Total reward: -48.875457763671875 Training loss: 0.8794 Explore P: 0.5490\n",
      "Episode: 134 Total reward: -54.2197265625 Training loss: 1.8690 Explore P: 0.5478\n",
      "Episode: 135 Total reward: -13.654006958007812 Training loss: 1.5325 Explore P: 0.5456\n",
      "Model Saved\n",
      "Episode: 136 Total reward: -62.413909912109375 Training loss: 15.5785 Explore P: 0.5434\n",
      "Episode: 137 Total reward: -54.82696533203125 Training loss: 1.0134 Explore P: 0.5412\n",
      "Episode: 138 Total reward: -48.35005187988281 Training loss: 25.8006 Explore P: 0.5391\n",
      "Episode: 139 Total reward: -74.66207885742188 Training loss: 1.4984 Explore P: 0.5380\n",
      "Episode: 140 Total reward: 32.18220520019531 Training loss: 18.2033 Explore P: 0.5358\n",
      "Model Saved\n",
      "Episode: 141 Total reward: -63.87110900878906 Training loss: 0.8813 Explore P: 0.5336\n",
      "Episode: 142 Total reward: -62.83335876464844 Training loss: 1.0049 Explore P: 0.5315\n",
      "Episode: 143 Total reward: -10.6583251953125 Training loss: 21.2732 Explore P: 0.5293\n",
      "Episode: 144 Total reward: -53.86592102050781 Training loss: 1.0365 Explore P: 0.5272\n",
      "Episode: 145 Total reward: 3.959625244140625 Training loss: 1.5841 Explore P: 0.5251\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 37.72615051269531 Training loss: 16.4014 Explore P: 0.5230\n",
      "Episode: 147 Total reward: -95.64239501953125 Training loss: 21.5894 Explore P: 0.5209\n",
      "Episode: 148 Total reward: 5.3899078369140625 Training loss: 1.1268 Explore P: 0.5151\n",
      "Episode: 149 Total reward: 3.687713623046875 Training loss: 0.6801 Explore P: 0.5131\n",
      "Episode: 150 Total reward: 7.630340576171875 Training loss: 0.5195 Explore P: 0.5110\n",
      "Model Saved\n",
      "Episode: 151 Total reward: -26.031463623046875 Training loss: 1.3604 Explore P: 0.5090\n",
      "Episode: 152 Total reward: 8.024032592773438 Training loss: 0.6916 Explore P: 0.5069\n",
      "Episode: 153 Total reward: -30.790176391601562 Training loss: 0.7955 Explore P: 0.5049\n",
      "Episode: 154 Total reward: -37.95552062988281 Training loss: 1.4570 Explore P: 0.5029\n",
      "Episode: 155 Total reward: -83.77711486816406 Training loss: 1.1745 Explore P: 0.5019\n",
      "Model Saved\n",
      "Episode: 156 Total reward: -26.694854736328125 Training loss: 13.9101 Explore P: 0.4999\n",
      "Episode: 157 Total reward: -95.81938171386719 Training loss: 19.7247 Explore P: 0.4988\n",
      "Episode: 158 Total reward: -55.6724853515625 Training loss: 42.5760 Explore P: 0.4968\n",
      "Episode: 159 Total reward: -83.24746704101562 Training loss: 1.5260 Explore P: 0.4949\n",
      "Episode: 160 Total reward: -48.96488952636719 Training loss: 1.1853 Explore P: 0.4929\n",
      "Model Saved\n",
      "Episode: 161 Total reward: -39.414337158203125 Training loss: 1.7108 Explore P: 0.4909\n",
      "Episode: 162 Total reward: -87.23104858398438 Training loss: 2.2232 Explore P: 0.4890\n",
      "Episode: 163 Total reward: -21.348541259765625 Training loss: 1.1891 Explore P: 0.4871\n",
      "Episode: 164 Total reward: -91.10240173339844 Training loss: 1.1491 Explore P: 0.4851\n",
      "Episode: 165 Total reward: -39.3643798828125 Training loss: 1.4939 Explore P: 0.4825\n",
      "Model Saved\n",
      "Episode: 166 Total reward: -45.03181457519531 Training loss: 1.1718 Explore P: 0.4806\n",
      "Episode: 167 Total reward: -72.02041625976562 Training loss: 1.7020 Explore P: 0.4786\n",
      "Episode: 168 Total reward: -0.503448486328125 Training loss: 1.1547 Explore P: 0.4767\n",
      "Episode: 169 Total reward: -11.7110595703125 Training loss: 1.3287 Explore P: 0.4741\n",
      "Episode: 170 Total reward: -48.70518493652344 Training loss: 0.9582 Explore P: 0.4722\n",
      "Model Saved\n",
      "Episode: 171 Total reward: -64.86520385742188 Training loss: 1.7665 Explore P: 0.4696\n",
      "Episode: 172 Total reward: -64.69512939453125 Training loss: 17.5337 Explore P: 0.4670\n",
      "Episode: 173 Total reward: -59.38960266113281 Training loss: 1.9980 Explore P: 0.4651\n",
      "Episode: 174 Total reward: -52.33332824707031 Training loss: 1.6985 Explore P: 0.4640\n",
      "Episode: 175 Total reward: -77.78462219238281 Training loss: 1.6090 Explore P: 0.4630\n",
      "Model Saved\n",
      "Episode: 176 Total reward: -65.09249877929688 Training loss: 2.2038 Explore P: 0.4611\n",
      "Episode: 177 Total reward: -52.7489013671875 Training loss: 1.4661 Explore P: 0.4592\n",
      "Episode: 178 Total reward: -1.6895294189453125 Training loss: 0.8405 Explore P: 0.4574\n",
      "Episode: 179 Total reward: -11.573715209960938 Training loss: 1.6399 Explore P: 0.4556\n",
      "Episode: 180 Total reward: -35.06925964355469 Training loss: 1.5614 Explore P: 0.4540\n",
      "Model Saved\n",
      "Episode: 181 Total reward: -61.02166748046875 Training loss: 2.2166 Explore P: 0.4522\n",
      "Episode: 182 Total reward: -53.86993408203125 Training loss: 46.3089 Explore P: 0.4504\n",
      "Episode: 183 Total reward: -99.24833679199219 Training loss: 1.9080 Explore P: 0.4488\n",
      "Episode: 184 Total reward: -27.28021240234375 Training loss: 2.7139 Explore P: 0.4470\n",
      "Episode: 185 Total reward: 15.091567993164062 Training loss: 2.6858 Explore P: 0.4452\n",
      "Model Saved\n",
      "Episode: 186 Total reward: -83.60908508300781 Training loss: 1.3247 Explore P: 0.4429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 187 Total reward: -12.4166259765625 Training loss: 7.6572 Explore P: 0.4411\n",
      "Episode: 188 Total reward: -57.97666931152344 Training loss: 2.3688 Explore P: 0.4400\n",
      "Episode: 189 Total reward: -73.35598754882812 Training loss: 1.3567 Explore P: 0.4390\n",
      "Episode: 190 Total reward: -72.44622802734375 Training loss: 1.0615 Explore P: 0.4380\n",
      "Model Saved\n",
      "Episode: 191 Total reward: -53.833221435546875 Training loss: 1.4156 Explore P: 0.4363\n",
      "Episode: 192 Total reward: -22.984634399414062 Training loss: 0.9412 Explore P: 0.4346\n",
      "Episode: 193 Total reward: -14.407135009765625 Training loss: 0.8770 Explore P: 0.4329\n",
      "Episode: 194 Total reward: -71.23860168457031 Training loss: 2.5495 Explore P: 0.4319\n",
      "Episode: 195 Total reward: 17.364242553710938 Training loss: 1.3053 Explore P: 0.4289\n",
      "Model Saved\n",
      "Episode: 196 Total reward: -30.22979736328125 Training loss: 1.2999 Explore P: 0.4272\n",
      "Episode: 197 Total reward: -88.61210632324219 Training loss: 1.7826 Explore P: 0.4254\n",
      "Episode: 198 Total reward: -62.95294189453125 Training loss: 2.6378 Explore P: 0.4231\n",
      "Episode: 199 Total reward: -37.39988708496094 Training loss: 10.7252 Explore P: 0.4215\n",
      "Episode: 200 Total reward: -34.57209777832031 Training loss: 5.6563 Explore P: 0.4198\n",
      "Model Saved\n",
      "Episode: 201 Total reward: -39.4503173828125 Training loss: 0.7497 Explore P: 0.4175\n",
      "Episode: 202 Total reward: -95.66044616699219 Training loss: 0.9827 Explore P: 0.4159\n",
      "Episode: 203 Total reward: -112.55506896972656 Training loss: 9.3529 Explore P: 0.4131\n",
      "Episode: 204 Total reward: -37.20025634765625 Training loss: 1.1824 Explore P: 0.4115\n",
      "Episode: 205 Total reward: -97.75062561035156 Training loss: 1.1464 Explore P: 0.4093\n",
      "Model Saved\n",
      "Episode: 206 Total reward: -34.23918151855469 Training loss: 11.0517 Explore P: 0.4077\n",
      "Episode: 207 Total reward: -41.813140869140625 Training loss: 8.8246 Explore P: 0.4061\n",
      "Episode: 208 Total reward: -48.31880187988281 Training loss: 1.7114 Explore P: 0.4052\n",
      "Episode: 209 Total reward: -28.139617919921875 Training loss: 2.3037 Explore P: 0.4024\n",
      "Episode: 210 Total reward: -52.162445068359375 Training loss: 2.4480 Explore P: 0.4009\n",
      "Model Saved\n",
      "Episode: 211 Total reward: -9.061233520507812 Training loss: 1.3890 Explore P: 0.3993\n",
      "Episode: 212 Total reward: -23.03759765625 Training loss: 25.1870 Explore P: 0.3977\n",
      "Episode: 213 Total reward: -74.29997253417969 Training loss: 1.7747 Explore P: 0.3956\n",
      "Episode: 214 Total reward: -91.68511962890625 Training loss: 0.8866 Explore P: 0.3941\n",
      "Episode: 215 Total reward: -82.83738708496094 Training loss: 1.7132 Explore P: 0.3932\n",
      "Model Saved\n",
      "Episode: 216 Total reward: -85.54428100585938 Training loss: 17.0213 Explore P: 0.3916\n",
      "Episode: 217 Total reward: -39.2310791015625 Training loss: 7.2292 Explore P: 0.3888\n",
      "Episode: 218 Total reward: -75.79783630371094 Training loss: 22.5413 Explore P: 0.3869\n",
      "Episode: 219 Total reward: -66.06675720214844 Training loss: 25.6360 Explore P: 0.3853\n",
      "Episode: 220 Total reward: -88.5311279296875 Training loss: 1.0714 Explore P: 0.3827\n",
      "Model Saved\n",
      "Episode: 221 Total reward: -103.23638916015625 Training loss: 31.5940 Explore P: 0.3818\n",
      "Episode: 222 Total reward: -102.52351379394531 Training loss: 18.7877 Explore P: 0.3804\n",
      "Episode: 223 Total reward: -98.33631896972656 Training loss: 16.3235 Explore P: 0.3783\n",
      "Episode: 224 Total reward: -52.475860595703125 Training loss: 1.2064 Explore P: 0.3774\n",
      "Episode: 225 Total reward: -72.79252624511719 Training loss: 5.9514 Explore P: 0.3759\n",
      "Model Saved\n",
      "Episode: 226 Total reward: -96.87643432617188 Training loss: 2.4697 Explore P: 0.3745\n",
      "Model updated\n",
      "Episode: 227 Total reward: -20.986923217773438 Training loss: 2.4274 Explore P: 0.3726\n",
      "Episode: 228 Total reward: -42.96415710449219 Training loss: 1.5285 Explore P: 0.3717\n",
      "Episode: 229 Total reward: -40.713714599609375 Training loss: 2.8368 Explore P: 0.3696\n",
      "Episode: 230 Total reward: 9.823410034179688 Training loss: 2.0038 Explore P: 0.3681\n",
      "Model Saved\n",
      "Episode: 231 Total reward: -99.72369384765625 Training loss: 11.2924 Explore P: 0.3666\n",
      "Episode: 232 Total reward: -40.3248291015625 Training loss: 18.2001 Explore P: 0.3652\n",
      "Episode: 233 Total reward: -85.39163208007812 Training loss: 1.6792 Explore P: 0.3637\n",
      "Episode: 234 Total reward: -26.16290283203125 Training loss: 15.8763 Explore P: 0.3617\n",
      "Episode: 235 Total reward: -48.825927734375 Training loss: 7.4001 Explore P: 0.3609\n",
      "Model Saved\n",
      "Episode: 236 Total reward: -94.91537475585938 Training loss: 19.6433 Explore P: 0.3594\n",
      "Episode: 237 Total reward: -109.60380554199219 Training loss: 10.5411 Explore P: 0.3582\n",
      "Episode: 238 Total reward: 2.26678466796875 Training loss: 3.8162 Explore P: 0.3568\n",
      "Episode: 239 Total reward: -4.2718963623046875 Training loss: 2.2107 Explore P: 0.3554\n",
      "Episode: 240 Total reward: -102.32046508789062 Training loss: 3.0788 Explore P: 0.3545\n",
      "Model Saved\n",
      "Episode: 241 Total reward: -22.28436279296875 Training loss: 25.4019 Explore P: 0.3531\n",
      "Episode: 242 Total reward: -47.516693115234375 Training loss: 2.4814 Explore P: 0.3517\n",
      "Episode: 243 Total reward: -100.93785095214844 Training loss: 1.3485 Explore P: 0.3509\n",
      "Episode: 244 Total reward: -43.82673645019531 Training loss: 5.8420 Explore P: 0.3495\n",
      "Episode: 245 Total reward: -59.76789855957031 Training loss: 1.6740 Explore P: 0.3481\n",
      "Model Saved\n",
      "Episode: 246 Total reward: -38.30070495605469 Training loss: 3.0597 Explore P: 0.3468\n",
      "Episode: 247 Total reward: -115.99856567382812 Training loss: 3.8446 Explore P: 0.3452\n",
      "Episode: 248 Total reward: -110.10588073730469 Training loss: 11.6549 Explore P: 0.3433\n",
      "Episode: 249 Total reward: -80.48060607910156 Training loss: 16.0106 Explore P: 0.3419\n",
      "Episode: 250 Total reward: -92.47459411621094 Training loss: 6.8312 Explore P: 0.3405\n",
      "Model Saved\n",
      "Episode: 251 Total reward: -35.93214416503906 Training loss: 1.3600 Explore P: 0.3398\n",
      "Episode: 252 Total reward: -97.11268615722656 Training loss: 1.3277 Explore P: 0.3385\n",
      "Episode: 253 Total reward: -85.24462890625 Training loss: 1.9839 Explore P: 0.3371\n",
      "Episode: 254 Total reward: -73.99270629882812 Training loss: 11.3529 Explore P: 0.3358\n",
      "Episode: 255 Total reward: -57.01683044433594 Training loss: 1.0390 Explore P: 0.3345\n",
      "Model Saved\n",
      "Episode: 256 Total reward: -102.43183898925781 Training loss: 13.5312 Explore P: 0.3332\n",
      "Episode: 257 Total reward: -85.58285522460938 Training loss: 2.3101 Explore P: 0.3311\n",
      "Episode: 258 Total reward: -66.03091430664062 Training loss: 9.9849 Explore P: 0.3298\n",
      "Episode: 259 Total reward: -66.77345275878906 Training loss: 20.2439 Explore P: 0.3279\n",
      "Episode: 260 Total reward: -92.68608093261719 Training loss: 26.9832 Explore P: 0.3273\n",
      "Model Saved\n",
      "Episode: 261 Total reward: -43.75396728515625 Training loss: 9.3230 Explore P: 0.3260\n",
      "Episode: 262 Total reward: -90.65151977539062 Training loss: 1.9929 Explore P: 0.3246\n",
      "Episode: 263 Total reward: -115.95594787597656 Training loss: 1.8722 Explore P: 0.3235\n",
      "Episode: 264 Total reward: -115.99928283691406 Training loss: 1.1333 Explore P: 0.3227\n",
      "Episode: 265 Total reward: -69.95063781738281 Training loss: 1.5427 Explore P: 0.3214\n",
      "Model Saved\n",
      "Episode: 266 Total reward: -46.892425537109375 Training loss: 2.4332 Explore P: 0.3198\n",
      "Episode: 267 Total reward: -115.62629699707031 Training loss: 1.9415 Explore P: 0.3185\n",
      "Episode: 268 Total reward: -85.11634826660156 Training loss: 3.6749 Explore P: 0.3173\n",
      "Episode: 269 Total reward: -90.74359130859375 Training loss: 0.8746 Explore P: 0.3165\n",
      "Episode: 270 Total reward: -41.288909912109375 Training loss: 2.2798 Explore P: 0.3153\n",
      "Model Saved\n",
      "Episode: 271 Total reward: -115.50970458984375 Training loss: 5.3738 Explore P: 0.3146\n",
      "Episode: 272 Total reward: -35.21461486816406 Training loss: 13.2894 Explore P: 0.3133\n",
      "Episode: 273 Total reward: -45.90509033203125 Training loss: 2.6191 Explore P: 0.3120\n",
      "Episode: 274 Total reward: -67.9683837890625 Training loss: 21.1558 Explore P: 0.3108\n",
      "Episode: 275 Total reward: -73.24082946777344 Training loss: 2.6422 Explore P: 0.3096\n",
      "Model Saved\n",
      "Episode: 276 Total reward: -111.82743835449219 Training loss: 2.8416 Explore P: 0.3084\n",
      "Episode: 277 Total reward: -109.33840942382812 Training loss: 11.6424 Explore P: 0.3072\n",
      "Episode: 278 Total reward: -84.5472412109375 Training loss: 5.6918 Explore P: 0.3060\n",
      "Episode: 279 Total reward: -104.97294616699219 Training loss: 1.5419 Explore P: 0.3043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 280 Total reward: -101.73876953125 Training loss: 1.2468 Explore P: 0.3031\n",
      "Model Saved\n",
      "Episode: 281 Total reward: -54.58653259277344 Training loss: 9.0259 Explore P: 0.3019\n",
      "Episode: 282 Total reward: -110.93882751464844 Training loss: 1.8144 Explore P: 0.2989\n",
      "Episode: 283 Total reward: -115.90835571289062 Training loss: 1.3201 Explore P: 0.2972\n",
      "Episode: 284 Total reward: -49.26557922363281 Training loss: 2.2046 Explore P: 0.2960\n",
      "Episode: 285 Total reward: -97.17985534667969 Training loss: 1.7569 Explore P: 0.2953\n",
      "Model Saved\n",
      "Episode: 286 Total reward: -33.01533508300781 Training loss: 1.3273 Explore P: 0.2941\n",
      "Episode: 287 Total reward: -74.89543151855469 Training loss: 21.1552 Explore P: 0.2930\n",
      "Episode: 288 Total reward: -79.28390502929688 Training loss: 1.5119 Explore P: 0.2918\n",
      "Episode: 289 Total reward: -84.67340087890625 Training loss: 3.0751 Explore P: 0.2898\n",
      "Episode: 290 Total reward: -74.93247985839844 Training loss: 22.4415 Explore P: 0.2886\n",
      "Model Saved\n",
      "Episode: 291 Total reward: -77.27525329589844 Training loss: 2.8056 Explore P: 0.2875\n",
      "Episode: 292 Total reward: -112.60409545898438 Training loss: 25.5458 Explore P: 0.2864\n",
      "Episode: 293 Total reward: -102.83503723144531 Training loss: 2.1457 Explore P: 0.2853\n",
      "Episode: 294 Total reward: -96.60554504394531 Training loss: 15.2651 Explore P: 0.2847\n",
      "Episode: 295 Total reward: -43.54139709472656 Training loss: 2.2195 Explore P: 0.2836\n",
      "Model Saved\n",
      "Episode: 296 Total reward: -62.83929443359375 Training loss: 1.6145 Explore P: 0.2824\n",
      "Episode: 297 Total reward: -70.3236083984375 Training loss: 10.7285 Explore P: 0.2813\n",
      "Episode: 298 Total reward: -100.67219543457031 Training loss: 1.5078 Explore P: 0.2793\n",
      "Episode: 299 Total reward: -102.61897277832031 Training loss: 1.3825 Explore P: 0.2783\n",
      "Episode: 300 Total reward: -35.305145263671875 Training loss: 1.1889 Explore P: 0.2772\n",
      "Model Saved\n",
      "Episode: 301 Total reward: -54.0263671875 Training loss: 29.1758 Explore P: 0.2756\n",
      "Episode: 302 Total reward: -53.42762756347656 Training loss: 3.5295 Explore P: 0.2745\n",
      "Episode: 303 Total reward: -70.60484313964844 Training loss: 13.9890 Explore P: 0.2734\n",
      "Episode: 304 Total reward: -62.13922119140625 Training loss: 12.0842 Explore P: 0.2724\n",
      "Episode: 305 Total reward: -87.04319763183594 Training loss: 1.0068 Explore P: 0.2713\n",
      "Model Saved\n",
      "Episode: 306 Total reward: -111.11335754394531 Training loss: 1.6049 Explore P: 0.2710\n",
      "Episode: 307 Total reward: -70.35159301757812 Training loss: 2.0502 Explore P: 0.2704\n",
      "Episode: 308 Total reward: -56.48817443847656 Training loss: 8.5627 Explore P: 0.2690\n",
      "Episode: 309 Total reward: -55.116668701171875 Training loss: 2.5360 Explore P: 0.2680\n",
      "Episode: 310 Total reward: -85.02774047851562 Training loss: 24.6912 Explore P: 0.2669\n",
      "Model Saved\n",
      "Episode: 311 Total reward: -27.486343383789062 Training loss: 25.8212 Explore P: 0.2658\n",
      "Episode: 312 Total reward: -11.10137939453125 Training loss: 25.6476 Explore P: 0.2648\n",
      "Episode: 313 Total reward: -115.93589782714844 Training loss: 19.7048 Explore P: 0.2638\n",
      "Episode: 314 Total reward: -43.0723876953125 Training loss: 2.2465 Explore P: 0.2627\n",
      "Episode: 315 Total reward: -101.79928588867188 Training loss: 9.6558 Explore P: 0.2617\n",
      "Model Saved\n",
      "Episode: 316 Total reward: -50.29890441894531 Training loss: 7.8180 Explore P: 0.2607\n",
      "Episode: 317 Total reward: -72.39561462402344 Training loss: 15.1261 Explore P: 0.2597\n",
      "Episode: 318 Total reward: -115.55253601074219 Training loss: 1.6934 Explore P: 0.2586\n",
      "Episode: 319 Total reward: -63.366119384765625 Training loss: 1.6782 Explore P: 0.2576\n",
      "Episode: 320 Total reward: -76.9068603515625 Training loss: 1.4022 Explore P: 0.2566\n",
      "Model Saved\n",
      "Episode: 321 Total reward: -88.19589233398438 Training loss: 1.3320 Explore P: 0.2556\n",
      "Episode: 322 Total reward: -63.381744384765625 Training loss: 1.5170 Explore P: 0.2546\n",
      "Episode: 323 Total reward: -42.07856750488281 Training loss: 3.1582 Explore P: 0.2536\n",
      "Episode: 324 Total reward: -65.88665771484375 Training loss: 1.6499 Explore P: 0.2526\n",
      "Episode: 325 Total reward: -56.966156005859375 Training loss: 2.4557 Explore P: 0.2516\n",
      "Model Saved\n",
      "Episode: 326 Total reward: -115.86613464355469 Training loss: 0.7313 Explore P: 0.2507\n",
      "Episode: 327 Total reward: -54.14952087402344 Training loss: 1.6218 Explore P: 0.2497\n",
      "Episode: 328 Total reward: -70.80731201171875 Training loss: 2.4740 Explore P: 0.2487\n",
      "Episode: 329 Total reward: -48.94029235839844 Training loss: 1.3857 Explore P: 0.2482\n",
      "Episode: 330 Total reward: -6.158203125 Training loss: 2.5737 Explore P: 0.2472\n",
      "Model Saved\n",
      "Episode: 331 Total reward: -49.83454895019531 Training loss: 4.0433 Explore P: 0.2463\n",
      "Episode: 332 Total reward: -55.28321838378906 Training loss: 1.6791 Explore P: 0.2453\n",
      "Episode: 333 Total reward: -45.16490173339844 Training loss: 1.4965 Explore P: 0.2444\n",
      "Episode: 334 Total reward: -50.299346923828125 Training loss: 1.3755 Explore P: 0.2427\n",
      "Episode: 335 Total reward: -114.72035217285156 Training loss: 15.9939 Explore P: 0.2401\n",
      "Model Saved\n",
      "Episode: 336 Total reward: -114.49919128417969 Training loss: 3.4154 Explore P: 0.2388\n",
      "Episode: 337 Total reward: -84.25112915039062 Training loss: 3.1525 Explore P: 0.2379\n",
      "Episode: 338 Total reward: -115.95606994628906 Training loss: 1.7696 Explore P: 0.2366\n",
      "Episode: 339 Total reward: -59.99797058105469 Training loss: 9.9206 Explore P: 0.2357\n",
      "Episode: 340 Total reward: -115.94340515136719 Training loss: 2.0132 Explore P: 0.2348\n",
      "Model Saved\n",
      "Episode: 341 Total reward: -17.643478393554688 Training loss: 16.0527 Explore P: 0.2339\n",
      "Episode: 342 Total reward: -104.35005187988281 Training loss: 2.9576 Explore P: 0.2330\n",
      "Episode: 343 Total reward: -115.94947814941406 Training loss: 4.7131 Explore P: 0.2325\n",
      "Episode: 344 Total reward: -50.218841552734375 Training loss: 1.0288 Explore P: 0.2316\n",
      "Model updated\n",
      "Episode: 345 Total reward: -113.07942199707031 Training loss: 4.2270 Explore P: 0.2307\n",
      "Model Saved\n",
      "Episode: 346 Total reward: -54.47468566894531 Training loss: 2.0038 Explore P: 0.2298\n",
      "Episode: 347 Total reward: -35.317535400390625 Training loss: 3.4030 Explore P: 0.2286\n",
      "Episode: 348 Total reward: -115.97563171386719 Training loss: 1.6870 Explore P: 0.2281\n",
      "Episode: 349 Total reward: -96.04216003417969 Training loss: 7.3789 Explore P: 0.2272\n",
      "Episode: 350 Total reward: -113.01945495605469 Training loss: 6.6032 Explore P: 0.2267\n",
      "Model Saved\n",
      "Episode: 351 Total reward: -115.97569274902344 Training loss: 1.8397 Explore P: 0.2258\n",
      "Episode: 352 Total reward: -115.98493957519531 Training loss: 20.6430 Explore P: 0.2249\n",
      "Episode: 353 Total reward: -97.14601135253906 Training loss: 2.4511 Explore P: 0.2237\n",
      "Episode: 354 Total reward: -83.84146118164062 Training loss: 2.5620 Explore P: 0.2229\n",
      "Episode: 355 Total reward: -80.74128723144531 Training loss: 5.9433 Explore P: 0.2220\n",
      "Model Saved\n",
      "Episode: 356 Total reward: -91.45646667480469 Training loss: 8.3208 Explore P: 0.2211\n",
      "Episode: 357 Total reward: -92.18159484863281 Training loss: 3.8138 Explore P: 0.2203\n",
      "Episode: 358 Total reward: -71.36236572265625 Training loss: 0.7806 Explore P: 0.2194\n",
      "Episode: 359 Total reward: -83.37710571289062 Training loss: 1.6962 Explore P: 0.2187\n",
      "Episode: 360 Total reward: -30.228546142578125 Training loss: 3.1704 Explore P: 0.2178\n",
      "Model Saved\n",
      "Episode: 361 Total reward: -68.46882629394531 Training loss: 1.4162 Explore P: 0.2170\n",
      "Episode: 362 Total reward: -64.29702758789062 Training loss: 1.6274 Explore P: 0.2161\n",
      "Episode: 363 Total reward: -115.98983764648438 Training loss: 4.2535 Explore P: 0.2153\n",
      "Episode: 364 Total reward: -94.09393310546875 Training loss: 2.9630 Explore P: 0.2145\n",
      "Episode: 365 Total reward: -106.18838500976562 Training loss: 2.5145 Explore P: 0.2136\n",
      "Model Saved\n",
      "Episode: 366 Total reward: -114.6776123046875 Training loss: 18.7716 Explore P: 0.2124\n",
      "Episode: 367 Total reward: -75.77365112304688 Training loss: 1.1713 Explore P: 0.2116\n",
      "Episode: 368 Total reward: -115.9791259765625 Training loss: 5.9440 Explore P: 0.2108\n",
      "Episode: 369 Total reward: -47.62103271484375 Training loss: 4.2621 Explore P: 0.2097\n",
      "Episode: 370 Total reward: -72.53251647949219 Training loss: 1.2622 Explore P: 0.2088\n",
      "Model Saved\n",
      "Episode: 371 Total reward: -96.34812927246094 Training loss: 2.2633 Explore P: 0.2080\n",
      "Episode: 372 Total reward: -115.97589111328125 Training loss: 18.4728 Explore P: 0.2072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 373 Total reward: -31.759246826171875 Training loss: 1.4598 Explore P: 0.2064\n",
      "Episode: 374 Total reward: -66.35745239257812 Training loss: 26.8616 Explore P: 0.2042\n",
      "Episode: 375 Total reward: -40.05412292480469 Training loss: 10.3995 Explore P: 0.2034\n",
      "Model Saved\n",
      "Episode: 376 Total reward: -46.139251708984375 Training loss: 1.9203 Explore P: 0.2026\n",
      "Episode: 377 Total reward: -82.57797241210938 Training loss: 2.0636 Explore P: 0.2020\n",
      "Episode: 378 Total reward: -38.8330078125 Training loss: 1.2759 Explore P: 0.2009\n",
      "Episode: 379 Total reward: -11.68304443359375 Training loss: 1.7240 Explore P: 0.1854\n",
      "Episode: 380 Total reward: -54.8214111328125 Training loss: 1.9578 Explore P: 0.1847\n",
      "Model Saved\n",
      "Episode: 381 Total reward: -84.83267211914062 Training loss: 1.2623 Explore P: 0.1840\n",
      "Episode: 382 Total reward: 32.855865478515625 Training loss: 1.4235 Explore P: 0.1833\n",
      "Episode: 383 Total reward: -0.874420166015625 Training loss: 1.2055 Explore P: 0.1826\n",
      "Episode: 384 Total reward: -30.160308837890625 Training loss: 12.2422 Explore P: 0.1818\n",
      "Episode: 385 Total reward: -115.97569274902344 Training loss: 4.4216 Explore P: 0.1811\n",
      "Model Saved\n",
      "Episode: 386 Total reward: -99.17790222167969 Training loss: 2.4208 Explore P: 0.1801\n",
      "Episode: 387 Total reward: -4.9401397705078125 Training loss: 2.6699 Explore P: 0.1794\n",
      "Episode: 388 Total reward: -34.576202392578125 Training loss: 1.0610 Explore P: 0.1788\n",
      "Episode: 389 Total reward: -46.17169189453125 Training loss: 2.2875 Explore P: 0.1783\n",
      "Episode: 390 Total reward: -91.87570190429688 Training loss: 21.0246 Explore P: 0.1776\n",
      "Model Saved\n",
      "Episode: 391 Total reward: -40.034393310546875 Training loss: 1.0774 Explore P: 0.1769\n",
      "Episode: 392 Total reward: 33.037139892578125 Training loss: 1.1812 Explore P: 0.1763\n",
      "Episode: 393 Total reward: -32.51165771484375 Training loss: 8.4694 Explore P: 0.1747\n",
      "Episode: 394 Total reward: -20.045211791992188 Training loss: 1.6184 Explore P: 0.1741\n",
      "Episode: 395 Total reward: 29.13958740234375 Training loss: 1.1218 Explore P: 0.1734\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 6.187896728515625 Training loss: 1.4358 Explore P: 0.1727\n",
      "Episode: 397 Total reward: -80.11714172363281 Training loss: 13.7952 Explore P: 0.1721\n",
      "Episode: 398 Total reward: -57.032806396484375 Training loss: 3.7052 Explore P: 0.1714\n",
      "Episode: 399 Total reward: -22.733657836914062 Training loss: 1.6538 Explore P: 0.1707\n",
      "Episode: 400 Total reward: -115.01310729980469 Training loss: 1.9644 Explore P: 0.1706\n",
      "Model Saved\n",
      "Episode: 401 Total reward: -30.204177856445312 Training loss: 23.9664 Explore P: 0.1697\n",
      "Episode: 402 Total reward: -18.446884155273438 Training loss: 3.5567 Explore P: 0.1691\n",
      "Episode: 403 Total reward: -87.11843872070312 Training loss: 13.7515 Explore P: 0.1684\n",
      "Episode: 404 Total reward: -66.97463989257812 Training loss: 2.7336 Explore P: 0.1678\n",
      "Episode: 405 Total reward: -115.98095703125 Training loss: 2.0746 Explore P: 0.1671\n",
      "Model Saved\n",
      "Episode: 406 Total reward: -68.43368530273438 Training loss: 9.0484 Explore P: 0.1665\n",
      "Episode: 407 Total reward: -26.035552978515625 Training loss: 3.1317 Explore P: 0.1659\n",
      "Episode: 408 Total reward: -41.93739318847656 Training loss: 1.8371 Explore P: 0.1652\n",
      "Episode: 409 Total reward: -77.98899841308594 Training loss: 1.9538 Explore P: 0.1649\n",
      "Episode: 410 Total reward: -4.7108612060546875 Training loss: 1.5673 Explore P: 0.1642\n",
      "Model Saved\n",
      "Episode: 411 Total reward: -78.81790161132812 Training loss: 1.4618 Explore P: 0.1636\n",
      "Episode: 412 Total reward: -37.237335205078125 Training loss: 18.6684 Explore P: 0.1630\n",
      "Episode: 413 Total reward: -110.49458312988281 Training loss: 12.4451 Explore P: 0.1624\n",
      "Episode: 414 Total reward: -110.76666259765625 Training loss: 1.8519 Explore P: 0.1618\n",
      "Episode: 415 Total reward: -92.51760864257812 Training loss: 1.4728 Explore P: 0.1615\n",
      "Model Saved\n",
      "Episode: 416 Total reward: -40.702301025390625 Training loss: 28.7030 Explore P: 0.1609\n",
      "Episode: 417 Total reward: -115.97569274902344 Training loss: 4.7522 Explore P: 0.1604\n",
      "Episode: 418 Total reward: -103.29399108886719 Training loss: 1.2676 Explore P: 0.1597\n",
      "Episode: 419 Total reward: -55.37187194824219 Training loss: 2.9801 Explore P: 0.1588\n",
      "Episode: 420 Total reward: -17.17938232421875 Training loss: 1.1062 Explore P: 0.1583\n",
      "Model Saved\n",
      "Episode: 421 Total reward: -27.3885498046875 Training loss: 2.2447 Explore P: 0.1577\n",
      "Episode: 422 Total reward: -15.034194946289062 Training loss: 2.0688 Explore P: 0.1570\n",
      "Episode: 423 Total reward: -10.752182006835938 Training loss: 6.6069 Explore P: 0.1564\n",
      "Episode: 424 Total reward: -115.97569274902344 Training loss: 2.1294 Explore P: 0.1556\n",
      "Episode: 425 Total reward: -68.55398559570312 Training loss: 24.1360 Explore P: 0.1548\n",
      "Model Saved\n",
      "Episode: 426 Total reward: -16.947235107421875 Training loss: 5.8836 Explore P: 0.1542\n",
      "Episode: 427 Total reward: -15.624160766601562 Training loss: 2.0533 Explore P: 0.1534\n",
      "Episode: 428 Total reward: -74.1085205078125 Training loss: 3.1483 Explore P: 0.1528\n",
      "Episode: 429 Total reward: -63.34700012207031 Training loss: 2.0456 Explore P: 0.1522\n",
      "Episode: 430 Total reward: -27.702590942382812 Training loss: 6.4735 Explore P: 0.1516\n",
      "Model Saved\n",
      "Episode: 431 Total reward: -21.413497924804688 Training loss: 1.7361 Explore P: 0.1511\n",
      "Episode: 432 Total reward: 6.2286529541015625 Training loss: 1.9546 Explore P: 0.1505\n",
      "Episode: 433 Total reward: -17.032852172851562 Training loss: 6.5804 Explore P: 0.1499\n",
      "Episode: 434 Total reward: -115.99130249023438 Training loss: 1.7719 Explore P: 0.1492\n",
      "Episode: 435 Total reward: 26.516006469726562 Training loss: 2.0172 Explore P: 0.1480\n",
      "Model Saved\n",
      "Episode: 436 Total reward: -66.28189086914062 Training loss: 1.6054 Explore P: 0.1475\n",
      "Episode: 437 Total reward: -8.914505004882812 Training loss: 15.0231 Explore P: 0.1469\n",
      "Episode: 438 Total reward: -58.549774169921875 Training loss: 1.6632 Explore P: 0.1463\n",
      "Episode: 439 Total reward: -13.749420166015625 Training loss: 8.1607 Explore P: 0.1458\n",
      "Episode: 440 Total reward: -33.08177185058594 Training loss: 22.3460 Explore P: 0.1454\n",
      "Model Saved\n",
      "Episode: 441 Total reward: -54.29328918457031 Training loss: 3.1751 Explore P: 0.1449\n",
      "Episode: 442 Total reward: -85.24644470214844 Training loss: 2.5175 Explore P: 0.1443\n",
      "Model updated\n",
      "Episode: 443 Total reward: -14.336624145507812 Training loss: 1.0980 Explore P: 0.1438\n",
      "Episode: 444 Total reward: 37.43829345703125 Training loss: 1.4653 Explore P: 0.1432\n",
      "Episode: 445 Total reward: -95.26077270507812 Training loss: 27.3352 Explore P: 0.1427\n",
      "Model Saved\n",
      "Episode: 446 Total reward: -18.594329833984375 Training loss: 2.1639 Explore P: 0.1422\n",
      "Episode: 447 Total reward: -11.605117797851562 Training loss: 3.4190 Explore P: 0.1416\n",
      "Episode: 448 Total reward: -21.721527099609375 Training loss: 22.9547 Explore P: 0.1409\n",
      "Episode: 449 Total reward: 14.21575927734375 Training loss: 2.2961 Explore P: 0.1403\n",
      "Episode: 450 Total reward: 63.15007019042969 Training loss: 5.3161 Explore P: 0.1398\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 67.57118225097656 Training loss: 1.4144 Explore P: 0.1393\n",
      "Episode: 452 Total reward: -58.656829833984375 Training loss: 2.9969 Explore P: 0.1388\n",
      "Episode: 453 Total reward: 40.387298583984375 Training loss: 1.1468 Explore P: 0.1382\n",
      "Episode: 454 Total reward: -38.709136962890625 Training loss: 2.8101 Explore P: 0.1377\n",
      "Episode: 455 Total reward: -36.2965087890625 Training loss: 1.6367 Explore P: 0.1372\n",
      "Model Saved\n",
      "Episode: 456 Total reward: -30.1202392578125 Training loss: 2.0580 Explore P: 0.1369\n",
      "Episode: 457 Total reward: 26.545730590820312 Training loss: 2.2179 Explore P: 0.1364\n",
      "Episode: 458 Total reward: -26.627243041992188 Training loss: 6.7268 Explore P: 0.1361\n",
      "Episode: 459 Total reward: 0.2383880615234375 Training loss: 3.3456 Explore P: 0.1358\n",
      "Episode: 460 Total reward: -57.07402038574219 Training loss: 2.0131 Explore P: 0.1353\n",
      "Model Saved\n",
      "Episode: 461 Total reward: -10.605972290039062 Training loss: 2.2414 Explore P: 0.1348\n",
      "Episode: 462 Total reward: -53.61474609375 Training loss: 4.2194 Explore P: 0.1343\n",
      "Episode: 463 Total reward: -72.28460693359375 Training loss: 10.8693 Explore P: 0.1340\n",
      "Episode: 464 Total reward: 8.260971069335938 Training loss: 2.1165 Explore P: 0.1335\n",
      "Episode: 465 Total reward: 17.388717651367188 Training loss: 2.1364 Explore P: 0.1330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 466 Total reward: 28.893386840820312 Training loss: 2.1610 Explore P: 0.1325\n",
      "Episode: 467 Total reward: -47.401611328125 Training loss: 1.0198 Explore P: 0.1320\n",
      "Episode: 468 Total reward: 27.089950561523438 Training loss: 6.4417 Explore P: 0.1315\n",
      "Episode: 469 Total reward: 21.407760620117188 Training loss: 21.6828 Explore P: 0.1310\n",
      "Episode: 470 Total reward: 22.998855590820312 Training loss: 6.8204 Explore P: 0.1305\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 11.813369750976562 Training loss: 13.5251 Explore P: 0.1300\n",
      "Episode: 472 Total reward: -51.877685546875 Training loss: 1.8424 Explore P: 0.1297\n",
      "Episode: 473 Total reward: 37.22886657714844 Training loss: 3.2896 Explore P: 0.1292\n",
      "Episode: 474 Total reward: -31.59661865234375 Training loss: 2.7693 Explore P: 0.1288\n",
      "Episode: 475 Total reward: -107.43583679199219 Training loss: 3.8901 Explore P: 0.1285\n",
      "Model Saved\n",
      "Episode: 476 Total reward: -12.372451782226562 Training loss: 2.5129 Explore P: 0.1282\n",
      "Episode: 477 Total reward: -28.414108276367188 Training loss: 25.5584 Explore P: 0.1277\n",
      "Episode: 478 Total reward: -1.1544189453125 Training loss: 13.5927 Explore P: 0.1270\n",
      "Episode: 479 Total reward: 37.586578369140625 Training loss: 14.1256 Explore P: 0.1264\n",
      "Episode: 480 Total reward: -10.27984619140625 Training loss: 2.7248 Explore P: 0.1259\n",
      "Model Saved\n",
      "Episode: 481 Total reward: -16.116775512695312 Training loss: 4.1725 Explore P: 0.1256\n",
      "Episode: 482 Total reward: 12.18756103515625 Training loss: 1.4472 Explore P: 0.1251\n",
      "Episode: 483 Total reward: -21.262359619140625 Training loss: 2.1316 Explore P: 0.1249\n",
      "Episode: 484 Total reward: 15.091766357421875 Training loss: 2.8841 Explore P: 0.1244\n",
      "Episode: 485 Total reward: -46.46147155761719 Training loss: 3.6370 Explore P: 0.1240\n",
      "Model Saved\n",
      "Episode: 486 Total reward: -115.87870788574219 Training loss: 13.8603 Explore P: 0.1239\n",
      "Episode: 487 Total reward: 11.220855712890625 Training loss: 1.1068 Explore P: 0.1235\n",
      "Episode: 488 Total reward: -29.204330444335938 Training loss: 9.2856 Explore P: 0.1232\n",
      "Episode: 489 Total reward: -3.9397735595703125 Training loss: 2.4285 Explore P: 0.1227\n",
      "Episode: 490 Total reward: 2.9941864013671875 Training loss: 9.1232 Explore P: 0.1223\n",
      "Model Saved\n",
      "Episode: 491 Total reward: -21.900833129882812 Training loss: 1.2258 Explore P: 0.1218\n",
      "Episode: 492 Total reward: -53.06373596191406 Training loss: 1.8493 Explore P: 0.1214\n",
      "Episode: 493 Total reward: 24.0992431640625 Training loss: 30.4316 Explore P: 0.1210\n",
      "Episode: 494 Total reward: -7.12384033203125 Training loss: 12.9986 Explore P: 0.1205\n",
      "Episode: 495 Total reward: -4.015838623046875 Training loss: 7.5225 Explore P: 0.1201\n",
      "Model Saved\n",
      "Episode: 496 Total reward: -82.65565490722656 Training loss: 1.9574 Explore P: 0.1200\n",
      "Episode: 497 Total reward: -18.619232177734375 Training loss: 3.1261 Explore P: 0.1196\n",
      "Episode: 498 Total reward: 29.144760131835938 Training loss: 4.8247 Explore P: 0.1191\n",
      "Episode: 499 Total reward: 32.444549560546875 Training loss: 1.3966 Explore P: 0.1187\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        tau = 0\n",
    "        game.init()\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(500):\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                tau += 1\n",
    "                decay_step +=1\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    next_state = np.zeros((120,140), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    step = max_steps\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                          'Total reward: {}'.format(total_reward),\n",
    "                          'Training loss: {:.4f}'.format(loss),\n",
    "                          'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    # 记忆存储\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    # 记忆存储\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                # 训练部分     \n",
    "                ## 获取记忆的mini_batch\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                \n",
    "                ### 行为网络计算 a'\n",
    "                ### 目标网络计算Q(s',a')\n",
    "                \n",
    "                ## 从行为网络获得 Q(s',a') 值\n",
    "                q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                ## 从目标网络获得 Q(s',a') 值\n",
    "                q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    ## 获取a'\n",
    "                    action = np.argmax(q_next_state[i])\n",
    "\n",
    "                    ## 如果是最终状态那么 Q_target = r\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    \n",
    "                    ## Q_target = r + gamma * Qtarget(s',a')\n",
    "                    else:\n",
    "                        ## 这里是 Double DQN 的 target 算法，使用来自行为网络的 action 下标，在目标网络里面进行再次的 Q 运算\n",
    "                        ## 实现了两套参数嵌套\n",
    "                        target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                               DQNetwork.target_Q: targets_mb,\n",
    "                                                               DQNetwork.actions_: actions_mb,\n",
    "                                                               DQNetwork.ISWeights_: ISWeights_mb})\n",
    "              \n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "                \n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                        DQNetwork.target_Q: targets_mb,\n",
    "                                                        DQNetwork.actions_: actions_mb,\n",
    "                                                        DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                if tau > max_tau:\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Model updated\")\n",
    "\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")\n",
    "                \n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: 观看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "Score:  -91.59956359863281\n",
      "Score:  -91.59956359863281\n",
      "Score:  -78.81681823730469\n",
      "Score:  -108.4110107421875\n",
      "Score:  -85.94677734375\n",
      "Score:  -91.59956359863281\n",
      "Score:  -91.59956359863281\n",
      "Score:  -91.59956359863281\n",
      "Score:  -91.59956359863281\n",
      "Score:  -91.51455688476562\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    game.load_config(\"deadly_corridor_testing.cfg\")\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    saver.restore(sess, \"models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            explore_probability = 0.01  \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                action = random.choice(possible_actions)\n",
    "        \n",
    "            else:\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
