{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 并不朴素的朴素贝叶斯\n",
    "Sklearn朴素贝叶斯官方连接[点击这里](http://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (iris.data.shape[0],(iris.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文语义分类\n",
    "\n",
    "### 创建训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chn_data = [[\"你个蠢蛋一天到晚啥也不干！\"],\n",
    "            [\"你的样子很疲劳，要注意多休息！\"],\n",
    "            [\"你这种打法很猥琐啊。\"],\n",
    "            [\"猥琐发育别发浪。\"],\n",
    "            [\"我太喜欢这部电影了。\"],\n",
    "            [\"这部电影真糟糕。\"]]\n",
    "\n",
    "chn_label = [0,1,0,1,1,0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab_set(chn_data):\n",
    "    vocab_list = []\n",
    "    for data in chn_data:\n",
    "        vocab_list += data[0]\n",
    "    vocab_list = list(set(vocab_list))\n",
    "    bag_list = [0]*len(vocab_list)\n",
    "    return vocab_list, bag_list\n",
    "\n",
    "def modify_data(chn_data, vocab_list, bag_list):\n",
    "    new_datas = []\n",
    "    for data in chn_data:\n",
    "        new_data = bag_list.copy()\n",
    "        for word in data[0]:\n",
    "            index = vocab_list.index(word)\n",
    "            new_data[index] = 1\n",
    "        new_datas.append(new_data)\n",
    "    return np.array(new_datas)\n",
    "\n",
    "vocab_list, bag_list = make_vocab_set(chn_data)\n",
    "train_data = modify_data(chn_data, vocab_list, bag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成模型并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chn_model = GaussianNB()\n",
    "chn_fit = chn_model.fit(train_data, chn_label)\n",
    "train_result = chn_fit.predict(train_data)\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"他长得猥琐\"\n",
    "test_label = 0\n",
    "def make_test_data(test_sentence, vocab_list, bag_list):\n",
    "    test_data = bag_list.copy()\n",
    "    for word in test_sentence:\n",
    "        if word in vocab_list:\n",
    "            index = vocab_list.index(word)\n",
    "            test_data[index] = 1\n",
    "    return np.array([test_data])\n",
    "\n",
    "test_data = make_test_data(test_sentence, vocab_list, bag_list)\n",
    "test_result = chn_fit.predict(test_data)\n",
    "print(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
